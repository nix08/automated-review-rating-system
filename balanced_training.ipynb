{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fafc561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171065</td>\n",
       "      <td>B00009ZIY2</td>\n",
       "      <td>A362H8YI6T7B2I</td>\n",
       "      <td>Patricia J Gilchrist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1322438400</td>\n",
       "      <td>not what we wanted...</td>\n",
       "      <td>We have two very picky cats who loved the old ...</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9366</td>\n",
       "      <td>B006N3IG4K</td>\n",
       "      <td>A1KJDUS91L5OOE</td>\n",
       "      <td>bears22687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343952000</td>\n",
       "      <td>not good</td>\n",
       "      <td>The coffee tasted bitter and like it was burnt...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410515</td>\n",
       "      <td>B000FIXYDC</td>\n",
       "      <td>A2C8O554YMY2ZL</td>\n",
       "      <td>Bixby \"Bixby\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1315785600</td>\n",
       "      <td>Strange taste and texture</td>\n",
       "      <td>I purchased a can that the store before I boug...</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>482561</td>\n",
       "      <td>B000G2UUDO</td>\n",
       "      <td>A145B9FRIAAHAY</td>\n",
       "      <td>Rita's talented tastebuds</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1278288000</td>\n",
       "      <td>AWFUL!!! AWFUL!!!</td>\n",
       "      <td>This product is a perfect example for the erro...</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120766</td>\n",
       "      <td>B005K4Q37A</td>\n",
       "      <td>A1UQFVHBQJ2K8Z</td>\n",
       "      <td>Brenda</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1325808000</td>\n",
       "      <td>Grove Square Cappuchino</td>\n",
       "      <td>I found that Grove Square French Vanilla Cappu...</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                ProfileName  \\\n",
       "0  171065  B00009ZIY2  A362H8YI6T7B2I       Patricia J Gilchrist   \n",
       "1    9366  B006N3IG4K  A1KJDUS91L5OOE                 bears22687   \n",
       "2  410515  B000FIXYDC  A2C8O554YMY2ZL              Bixby \"Bixby\"   \n",
       "3  482561  B000G2UUDO  A145B9FRIAAHAY  Rita's talented tastebuds   \n",
       "4  120766  B005K4Q37A  A1UQFVHBQJ2K8Z                     Brenda   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      1  1322438400   \n",
       "1                     0                       1      1  1343952000   \n",
       "2                     1                       2      1  1315785600   \n",
       "3                     3                       6      1  1278288000   \n",
       "4                     2                       5      1  1325808000   \n",
       "\n",
       "                     Summary  \\\n",
       "0      not what we wanted...   \n",
       "1                   not good   \n",
       "2  Strange taste and texture   \n",
       "3          AWFUL!!! AWFUL!!!   \n",
       "4    Grove Square Cappuchino   \n",
       "\n",
       "                                                Text  review_length  \n",
       "0  We have two very picky cats who loved the old ...            343  \n",
       "1  The coffee tasted bitter and like it was burnt...            136  \n",
       "2  I purchased a can that the store before I boug...            343  \n",
       "3  This product is a perfect example for the erro...           1096  \n",
       "4  I found that Grove Square French Vanilla Cappu...            430  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Download both 'punkt' and 'punkt_tab' – this is now required!\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')  # (for lemmatization, as before)\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "\n",
    "# File paths\n",
    "DATA_PATH = r\"D:\\Projects\\automated-review-rating-system\\data\\cleaned_dataset\\balanced_data.csv\"\n",
    "BALANCED_SAVE_PATH = r\"D:\\Projects\\automated-review-rating-system\\data\\cleaned_dataset\\balanced_data_lemmatized.csv\"\n",
    "\n",
    "# Column names\n",
    "REVIEW_COL = \"Text\"\n",
    "LABEL_COL = \"Score\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe66096",
   "metadata": {},
   "source": [
    "Data Cleaning\n",
    "We remove unwanted characters and lower-case the review text for uniformity. Missing values are handled as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9ac752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171065</td>\n",
       "      <td>B00009ZIY2</td>\n",
       "      <td>A362H8YI6T7B2I</td>\n",
       "      <td>Patricia J Gilchrist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1322438400</td>\n",
       "      <td>not what we wanted...</td>\n",
       "      <td>we have two very picky cats who loved the old ...</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9366</td>\n",
       "      <td>B006N3IG4K</td>\n",
       "      <td>A1KJDUS91L5OOE</td>\n",
       "      <td>bears22687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343952000</td>\n",
       "      <td>not good</td>\n",
       "      <td>the coffee tasted bitter and like it was burnt...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410515</td>\n",
       "      <td>B000FIXYDC</td>\n",
       "      <td>A2C8O554YMY2ZL</td>\n",
       "      <td>Bixby \"Bixby\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1315785600</td>\n",
       "      <td>Strange taste and texture</td>\n",
       "      <td>i purchased a can that the store before i boug...</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>482561</td>\n",
       "      <td>B000G2UUDO</td>\n",
       "      <td>A145B9FRIAAHAY</td>\n",
       "      <td>Rita's talented tastebuds</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1278288000</td>\n",
       "      <td>AWFUL!!! AWFUL!!!</td>\n",
       "      <td>this product is a perfect example for the erro...</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120766</td>\n",
       "      <td>B005K4Q37A</td>\n",
       "      <td>A1UQFVHBQJ2K8Z</td>\n",
       "      <td>Brenda</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1325808000</td>\n",
       "      <td>Grove Square Cappuchino</td>\n",
       "      <td>i found that grove square french vanilla cappu...</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                ProfileName  \\\n",
       "0  171065  B00009ZIY2  A362H8YI6T7B2I       Patricia J Gilchrist   \n",
       "1    9366  B006N3IG4K  A1KJDUS91L5OOE                 bears22687   \n",
       "2  410515  B000FIXYDC  A2C8O554YMY2ZL              Bixby \"Bixby\"   \n",
       "3  482561  B000G2UUDO  A145B9FRIAAHAY  Rita's talented tastebuds   \n",
       "4  120766  B005K4Q37A  A1UQFVHBQJ2K8Z                     Brenda   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      1  1322438400   \n",
       "1                     0                       1      1  1343952000   \n",
       "2                     1                       2      1  1315785600   \n",
       "3                     3                       6      1  1278288000   \n",
       "4                     2                       5      1  1325808000   \n",
       "\n",
       "                     Summary  \\\n",
       "0      not what we wanted...   \n",
       "1                   not good   \n",
       "2  Strange taste and texture   \n",
       "3          AWFUL!!! AWFUL!!!   \n",
       "4    Grove Square Cappuchino   \n",
       "\n",
       "                                                Text  review_length  \n",
       "0  we have two very picky cats who loved the old ...            343  \n",
       "1  the coffee tasted bitter and like it was burnt...            136  \n",
       "2  i purchased a can that the store before i boug...            343  \n",
       "3  this product is a perfect example for the erro...           1096  \n",
       "4  i found that grove square french vanilla cappu...            430  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = ''.join(char if char.isalpha() or char.isspace() else ' ' for char in text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "df[REVIEW_COL] = df[REVIEW_COL].apply(clean_text)\n",
    "df = df.dropna(subset=[REVIEW_COL, LABEL_COL]).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2691d",
   "metadata": {},
   "source": [
    "Text Lemmatization\n",
    "Lemmatize each review to reduce words to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41aaf69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\HP/nltk_data', 'c:\\\\Users\\\\HP\\\\anaconda3\\\\nltk_data', 'c:\\\\Users\\\\HP\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\Users\\\\HP\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>review_length</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171065</td>\n",
       "      <td>B00009ZIY2</td>\n",
       "      <td>A362H8YI6T7B2I</td>\n",
       "      <td>Patricia J Gilchrist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1322438400</td>\n",
       "      <td>not what we wanted...</td>\n",
       "      <td>we have two very picky cats who loved the old ...</td>\n",
       "      <td>343</td>\n",
       "      <td>we have two very picky cat who loved the old o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9366</td>\n",
       "      <td>B006N3IG4K</td>\n",
       "      <td>A1KJDUS91L5OOE</td>\n",
       "      <td>bears22687</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343952000</td>\n",
       "      <td>not good</td>\n",
       "      <td>the coffee tasted bitter and like it was burnt...</td>\n",
       "      <td>136</td>\n",
       "      <td>the coffee tasted bitter and like it wa burnt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410515</td>\n",
       "      <td>B000FIXYDC</td>\n",
       "      <td>A2C8O554YMY2ZL</td>\n",
       "      <td>Bixby \"Bixby\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1315785600</td>\n",
       "      <td>Strange taste and texture</td>\n",
       "      <td>i purchased a can that the store before i boug...</td>\n",
       "      <td>343</td>\n",
       "      <td>i purchased a can that the store before i boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>482561</td>\n",
       "      <td>B000G2UUDO</td>\n",
       "      <td>A145B9FRIAAHAY</td>\n",
       "      <td>Rita's talented tastebuds</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1278288000</td>\n",
       "      <td>AWFUL!!! AWFUL!!!</td>\n",
       "      <td>this product is a perfect example for the erro...</td>\n",
       "      <td>1096</td>\n",
       "      <td>this product is a perfect example for the erro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>120766</td>\n",
       "      <td>B005K4Q37A</td>\n",
       "      <td>A1UQFVHBQJ2K8Z</td>\n",
       "      <td>Brenda</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1325808000</td>\n",
       "      <td>Grove Square Cappuchino</td>\n",
       "      <td>i found that grove square french vanilla cappu...</td>\n",
       "      <td>430</td>\n",
       "      <td>i found that grove square french vanilla cappu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId                ProfileName  \\\n",
       "0  171065  B00009ZIY2  A362H8YI6T7B2I       Patricia J Gilchrist   \n",
       "1    9366  B006N3IG4K  A1KJDUS91L5OOE                 bears22687   \n",
       "2  410515  B000FIXYDC  A2C8O554YMY2ZL              Bixby \"Bixby\"   \n",
       "3  482561  B000G2UUDO  A145B9FRIAAHAY  Rita's talented tastebuds   \n",
       "4  120766  B005K4Q37A  A1UQFVHBQJ2K8Z                     Brenda   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      1  1322438400   \n",
       "1                     0                       1      1  1343952000   \n",
       "2                     1                       2      1  1315785600   \n",
       "3                     3                       6      1  1278288000   \n",
       "4                     2                       5      1  1325808000   \n",
       "\n",
       "                     Summary  \\\n",
       "0      not what we wanted...   \n",
       "1                   not good   \n",
       "2  Strange taste and texture   \n",
       "3          AWFUL!!! AWFUL!!!   \n",
       "4    Grove Square Cappuchino   \n",
       "\n",
       "                                                Text  review_length  \\\n",
       "0  we have two very picky cats who loved the old ...            343   \n",
       "1  the coffee tasted bitter and like it was burnt...            136   \n",
       "2  i purchased a can that the store before i boug...            343   \n",
       "3  this product is a perfect example for the erro...           1096   \n",
       "4  i found that grove square french vanilla cappu...            430   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  we have two very picky cat who loved the old o...  \n",
       "1  the coffee tasted bitter and like it wa burnt ...  \n",
       "2  i purchased a can that the store before i boug...  \n",
       "3  this product is a perfect example for the erro...  \n",
       "4  i found that grove square french vanilla cappu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(nltk.data.path)  # Check where NLTK is looking for resources\n",
    "nltk.download('punkt', quiet=False, force=True)\n",
    "nltk.download('wordnet', quiet=False, force=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "df[\"lemmatized_text\"] = df[REVIEW_COL].apply(lemmatize_text)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfe7d6",
   "metadata": {},
   "source": [
    "Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a8d928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>review_length</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>298500</td>\n",
       "      <td>B006N3I0DM</td>\n",
       "      <td>A254CKI391CFLO</td>\n",
       "      <td>Michael</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1300838400</td>\n",
       "      <td>Emeril's Bold - great cup of coffee</td>\n",
       "      <td>i keep this in my regular rotation of keurig b...</td>\n",
       "      <td>201</td>\n",
       "      <td>i keep this in my regular rotation of keurig b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>121863</td>\n",
       "      <td>B0014X8WIE</td>\n",
       "      <td>AJ5XH2V209QFJ</td>\n",
       "      <td>Donald L. Nagle</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1254960000</td>\n",
       "      <td>Very Vanilla</td>\n",
       "      <td>now that this is made in mexico it s much swee...</td>\n",
       "      <td>159</td>\n",
       "      <td>now that this is made in mexico it s much swee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>535050</td>\n",
       "      <td>B003DSBCZS</td>\n",
       "      <td>A1L6T8GSSV7BC1</td>\n",
       "      <td>grant</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1339545600</td>\n",
       "      <td>not as discribed</td>\n",
       "      <td>these are not as described these are not the c...</td>\n",
       "      <td>145</td>\n",
       "      <td>these are not a described these are not the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423930</td>\n",
       "      <td>B0067TI2EY</td>\n",
       "      <td>A1R4IVSZIDVGFZ</td>\n",
       "      <td>Alison Tripp</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1326067200</td>\n",
       "      <td>Starbucks K cup</td>\n",
       "      <td>disappointed in the product overly expensive a...</td>\n",
       "      <td>142</td>\n",
       "      <td>disappointed in the product overly expensive a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13810</td>\n",
       "      <td>B0008D6XH8</td>\n",
       "      <td>A2Q1OFIKIEOYGD</td>\n",
       "      <td>MendoMama \"MendoMama\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1349568000</td>\n",
       "      <td>Adds a nice flavor to plain drinking water.</td>\n",
       "      <td>i use this lemon oil to add a few drops to my ...</td>\n",
       "      <td>303</td>\n",
       "      <td>i use this lemon oil to add a few drop to my d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId          UserId            ProfileName  \\\n",
       "0  298500  B006N3I0DM  A254CKI391CFLO                Michael   \n",
       "1  121863  B0014X8WIE   AJ5XH2V209QFJ        Donald L. Nagle   \n",
       "2  535050  B003DSBCZS  A1L6T8GSSV7BC1                  grant   \n",
       "3  423930  B0067TI2EY  A1R4IVSZIDVGFZ           Alison Tripp   \n",
       "4   13810  B0008D6XH8  A2Q1OFIKIEOYGD  MendoMama \"MendoMama\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      4  1300838400   \n",
       "1                     3                       3      3  1254960000   \n",
       "2                     0                       0      1  1339545600   \n",
       "3                     1                       1      3  1326067200   \n",
       "4                     0                       0      3  1349568000   \n",
       "\n",
       "                                       Summary  \\\n",
       "0          Emeril's Bold - great cup of coffee   \n",
       "1                                 Very Vanilla   \n",
       "2                             not as discribed   \n",
       "3                              Starbucks K cup   \n",
       "4  Adds a nice flavor to plain drinking water.   \n",
       "\n",
       "                                                Text  review_length  \\\n",
       "0  i keep this in my regular rotation of keurig b...            201   \n",
       "1  now that this is made in mexico it s much swee...            159   \n",
       "2  these are not as described these are not the c...            145   \n",
       "3  disappointed in the product overly expensive a...            142   \n",
       "4  i use this lemon oil to add a few drops to my ...            303   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  i keep this in my regular rotation of keurig b...  \n",
       "1  now that this is made in mexico it s much swee...  \n",
       "2  these are not a described these are not the ch...  \n",
       "3  disappointed in the product overly expensive a...  \n",
       "4  i use this lemon oil to add a few drop to my d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "max_count = df[LABEL_COL].value_counts().max()\n",
    "for label in df[LABEL_COL].unique():\n",
    "    df_label = df[df[LABEL_COL] == label]\n",
    "    df_upsampled = resample(df_label, replace=True, n_samples=max_count, random_state=42)\n",
    "    dfs.append(df_upsampled)\n",
    "df_balanced = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_balanced[LABEL_COL].value_counts()\n",
    "df_balanced.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c33c6",
   "metadata": {},
   "source": [
    "## Save the Processed Balanced Dataset\n",
    "Export the cleaned, lemmatized, and balanced dataset for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff4cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset saved to: D:\\Projects\\automated-review-rating-system\\data\\cleaned_dataset\\balanced_data_lemmatized.csv\n"
     ]
    }
   ],
   "source": [
    "save_columns = [REVIEW_COL, \"lemmatized_text\", LABEL_COL]\n",
    "df_balanced.to_csv(BALANCED_SAVE_PATH, index=False, columns=save_columns)\n",
    "print(f\"Balanced dataset saved to: {BALANCED_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ddfbc",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b4d0852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  i keep this in my regular rotation of keurig b...   \n",
      "1  now that this is made in mexico it s much swee...   \n",
      "2  these are not as described these are not the c...   \n",
      "3  disappointed in the product overly expensive a...   \n",
      "4  i use this lemon oil to add a few drops to my ...   \n",
      "\n",
      "                                          clean_text  \\\n",
      "0  i keep this in my regular rotation of keurig b...   \n",
      "1  now that this is made in mexico it s much swee...   \n",
      "2  these are not as described these are not the c...   \n",
      "3  disappointed in the product overly expensive a...   \n",
      "4  i use this lemon oil to add a few drops to my ...   \n",
      "\n",
      "                                          lemmatized  \n",
      "0  regular rotation keurig bold coffee amazon sub...  \n",
      "1  mexico sweet old nesquick vanilla like strong ...  \n",
      "2  describe cherry fill gum product taste terribl...  \n",
      "3  disappoint product overly expensive taste like...  \n",
      "4  use lemon oil add drop drinking water refreshi...  \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Lowercase, remove URLs, HTML tags, emojis, punctuation, non-ASCII, digits, extra whitespace.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) # Remove emojis/unicode\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)        # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)            # Remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize using spaCy, remove stopwords, keep alphabetic tokens longer than 1 character.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and len(token.lemma_) > 1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_balanced['clean_text'] = df_balanced['Text'].apply(clean_text)\n",
    "df_balanced['lemmatized'] = df_balanced['clean_text'].apply(lemmatize_text)\n",
    "print(df_balanced[['Text', 'clean_text', 'lemmatized']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbbb99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text',\n",
      "       'review_length', 'lemmatized_text', 'clean_text', 'lemmatized'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_balanced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a4cd4",
   "metadata": {},
   "source": [
    " Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239466fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution:\n",
      " Score\n",
      "1    1600\n",
      "4    1600\n",
      "3    1600\n",
      "2    1600\n",
      "5    1600\n",
      "Name: count, dtype: int64\n",
      "Test distribution:\n",
      " Score\n",
      "1    400\n",
      "2    400\n",
      "3    400\n",
      "5    400\n",
      "4    400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_balanced['lemmatized']  # Using lemmatized column\n",
    "y = df_balanced['Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Train distribution:\\n\", y_train.value_counts())\n",
    "print(\"Test distribution:\\n\", y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4196c6e",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e979a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: (8000, 12138)\n",
      "TF-IDF test shape: (2000, 12138)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF train shape:\", X_train_vec.shape)\n",
    "print(\"TF-IDF test shape:\", X_test_vec.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45a95d",
   "metadata": {},
   "source": [
    "Model Training with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1d833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1      0.720     0.715     0.718       400\n",
      "           2      0.635     0.625     0.630       400\n",
      "           3      0.628     0.603     0.615       400\n",
      "           4      0.619     0.625     0.622       400\n",
      "           5      0.696     0.733     0.714       400\n",
      "\n",
      "    accuracy                          0.660      2000\n",
      "   macro avg      0.659     0.660     0.660      2000\n",
      "weighted avg      0.659     0.660     0.660      2000\n",
      "\n",
      "Confusion Matrix:\n",
      " [[286  53  19  16  26]\n",
      " [ 61 250  54  22  13]\n",
      " [ 34  45 241  55  25]\n",
      " [  9  31  46 250  64]\n",
      " [  7  15  24  61 293]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  \n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a789172",
   "metadata": {},
   "source": [
    "Hyper Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49905075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Best Hyperparameters: {'C': 10, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.79      0.79       400\n",
      "           2       0.71      0.72      0.72       400\n",
      "           3       0.72      0.73      0.73       400\n",
      "           4       0.72      0.74      0.73       400\n",
      "           5       0.79      0.77      0.78       400\n",
      "\n",
      "    accuracy                           0.75      2000\n",
      "   macro avg       0.75      0.75      0.75      2000\n",
      "weighted avg       0.75      0.75      0.75      2000\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[314  35  20  13  18]\n",
      " [ 41 288  36  24  11]\n",
      " [ 18  36 294  31  21]\n",
      " [ 15  24  36 295  30]\n",
      " [  4  20  25  45 306]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X = df_balanced['lemmatized']\n",
    "y = df_balanced['Score']\n",
    "\n",
    "# Apply SMOTE only to numeric vectors (for balanced training)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],        # Norm type\n",
    "    'solver': ['liblinear'],        # Solver that supports l1 and l2 penalties\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7️⃣ Fit the best model on balanced data\n",
    "grid_search.fit(X_train_bal, y_train_bal)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predictions on test set (using original test vectorized data)\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c601dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model A and its vectorizer have been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = joblib.load('vectorizer_model_A.pkl')\n",
    "\n",
    "# Save the best trained balanced model as Model A\n",
    "joblib.dump(best_model, 'model_A_balanced.pkl')\n",
    "\n",
    "# Save the TF-IDF vectorizer used for training\n",
    "joblib.dump(vectorizer, 'vectorizer_model_A.pkl')\n",
    "\n",
    "print(\"✅ Model A and its vectorizer have been saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab7d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
